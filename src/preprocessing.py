import tqdm
import gensim


def naive_tokenisation(data):
    data = data.lower()
    punctuation = ['"', 'â€' "'", "â€˜",'#','$','%','(',')','[',']','{','}','*','+',',','-','_', '/','\\',':',';','<','>','@','^','`','\xa0','Â¡','Â¢','Â£','Â¥','Â§','Â¨','Â«','Â¬','Â®','Â¯','Â°','Â±','Â²','Â³','Â´','Âµ','Â·','Â¹','Â»','Â¼','Â½','Â¾','Â¿','Ã—','â‚¤','â‚¦','â‚ª','â‚¬','â„‚','â„“','â„','â„¤','â…”','â…›','â†‘','â†’','â†”','â‡Œ','â‡’','â‡”','âˆ€','âˆ…','âˆˆ','âˆ','âˆ‘','âˆ’','âˆ–','âˆ—','âˆ˜','âˆš','âˆ','âˆ','âˆ¥','âˆ¦','âˆ§','âˆ¨','âˆ©','âˆª','âˆ«','â‰ƒ','â‰…','â‰ˆ','â‰”','â‰˜','â‰','â‰ ','â‰¡','â‰¤','â‰¥','â‰ª','âŠƒ','âŠ†','â‹…','â‹•','âŒ€','âŒŠ','âŒ‹','â”€','â–ˆ','â–³','â˜†','â˜‰','â™€','â™‚','â™ˆ','â™‰','â™Š','â™‹','â™Œ','â™','â™','â™','â™','â™‘','â™’','â™“','â™','â™¥','â™¦','â™©','â™ª','â™«','â™¬','â™­','âŸ¨','âŸ©','âŸº','â±±','â²','â²‰','â²','â²“','â²•','â²—','â²™','â²›','â²Ÿ','â²£','â²§','â²©','ã€','ãƒ»','ï¬€','ï¬','ï¬‚','ï¬…','ï¬†','ï¼ˆ','ï¼‰','ï¼Œ','ï¼›','ï¿½','ğŸ’–','â€“', ',', 'ËŒ', 'É›', 'Ë', '|', '\u2060', 'â€¢', '\u200e', '\ufeff', 'â€³', 'â€²', 'ËŠ', '\u200b', 'â‚‚', 'â‚‡', 'â‚ƒ', 'Ê»', 'â€¦', '\u200c', '\x92', '\x96', '~', '\u2009', 'â€’', '\u202a', '\u202c', '\u2002']
    for punct in tqdm.tqdm(punctuation):
        data = data.replace(punct, '')
    data = data.replace('!', '.').replace('?', '.')

    paragraphs = data.split('\n')
    sentences = []
    for p in tqdm.tqdm(paragraphs):
        if len(p) != 0:
            sentences_in_paragraph = p.split('. ')
            for sentence in sentences_in_paragraph:
                sentences.append([])
                for word in sentence.replace('.', '').split(' '):
                    if word not in ['', ' ']:
                        sentences[-1].append(word)

    tokenised_data = '\n'.join([' '.join(sentence) for sentence in sentences])
    return tokenised_data


def gensim_tokenisation(data):
    sentences = gensim.summarization.textcleaner.get_sentences(data)
    all_sentences = []
    for sentence in sentences:
        sentence_tokenized = gensim.utils.tokenize(sentence, deacc=False, lowercase=True)
        all_sentences.append([word for word in sentence_tokenized])

    tokenised_data = '\n'.join([' '.join(sentence) for sentence in all_sentences])
    return tokenised_data
